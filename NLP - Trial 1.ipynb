{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.Data ETL</b>\n",
    "\n",
    "<b>a. First, filter out all rows where the sentiment (‘context’) is not in the list of\n",
    "aforementioned list of sentiments i.e. {'sad', 'jealous', 'joyful', 'terrified'}. </b><br>\n",
    "In order to achieve this we will first reach the train and valid csv file and read it to the dataframe. Post this we will identify the rows where the context is <i>'sad', 'jealous', 'joyful', 'terrified'</i> and put it in a new dataframe. Post this we will merege the train and valid dataframes into a train_data dataframe<br>\n",
    "\n",
    "<b>b. Synthesize your training attributes and label</b><br>\n",
    "To achieve this we will extract only the Utterance and Context from the train_data and pass it as Attribute and Label to a new dataframe called final_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2355: expected 8 fields, saw 10\\nSkipping line 36628: expected 8 fields, saw 12\\nSkipping line 49433: expected 8 fields, saw 10\\nSkipping line 56957: expected 8 fields, saw 10\\nSkipping line 65019: expected 8 fields, saw 10\\n'\n",
      "b'Skipping line 4342: expected 8 fields, saw 10\\n'\n",
      "b'Skipping line 2168: expected 8 fields, saw 10\\n'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Performing Data ETL\n",
    "--------------------------------------------\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_list = {'sad', 'jealous', 'joyful', 'terrified'}\n",
    "\n",
    "#Read the csv files using the relative path\n",
    "train_file = r'C:\\Users\\Sanjana\\Downloads\\empatheticdialogues\\empatheticdialogues\\train.csv'\n",
    "valid_file = r'C:\\Users\\Sanjana\\Downloads\\empatheticdialogues\\empatheticdialogues\\valid.csv'\n",
    "test_file = r'C:\\Users\\Sanjana\\Downloads\\empatheticdialogues\\empatheticdialogues\\test.csv' \n",
    "\n",
    "#read the csv files and store them in the dataframe also exclude the errored/bad lines\n",
    "df_train = pd.read_csv(train_file, error_bad_lines=False)\n",
    "df_valid = pd.read_csv(valid_file, error_bad_lines=False)\n",
    "df_test = pd.read_csv(test_file, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate data where context is 'sad', 'jealous', 'joyful', 'terrified'\n",
    "train_data = df_train[(df_train.context == 'sad')|(df_train.context == 'jealous')|\n",
    "                      (df_train.context == 'joyful')|(df_train.context == 'terrified')]\n",
    "\n",
    "#Get Test data\n",
    "test_data = df_test[(df_test.context == 'sad')|(df_test.context == 'jealous')|\n",
    "                      (df_test.context == 'joyful')|(df_test.context == 'terrified')]\n",
    "\n",
    "#Merge the test and validation dataset\n",
    "train_data = train_data.append(df_valid[(df_valid.context == 'sad')|(df_valid.context == 'jealous')|\n",
    "                      (df_valid.context == 'joyful')|(df_valid.context == 'terrified')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Synthesize the attribute and labels\n",
    "--------------------------------------------\n",
    "'''\n",
    "#Train Dataset\n",
    "final_train_dataset = pd.DataFrame()\n",
    "final_train_dataset['Attribute'] = train_data['utterance']\n",
    "final_train_dataset['Label'] = train_data['context']\n",
    "\n",
    "#Test Dataset\n",
    "final_test_dataset = pd.DataFrame()\n",
    "final_test_dataset['Attribute'] = test_data['utterance']\n",
    "final_test_dataset['Label'] = test_data['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (10939, 2)\n",
      "Test dataset shape: (1349, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset shape: '+str(final_train_dataset.shape))\n",
    "print('Test dataset shape: '+str(final_test_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Convert the utterances into a sparse bag-of-words (BOW) </b><br>\n",
    "\n",
    "In order to achieve this\n",
    "1. Take the attribute column from final_train_dataset.Attribute and convert it to a sentence array\n",
    "2. Ensure to lowercase it for easy comparision\n",
    "3. Use the sklearn.CountVectorizer to count the occurances of words and form a matriz of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job interviews always make me sweat bullets_comma_ makes me uncomfortable in general to be looked at under a microscope like that. ',\n",
       " \"don't be nervous. just be prepared.\",\n",
       " 'i feel like getting prepared and then having a curve ball thrown at you throws you off. ',\n",
       " 'yes but if you stay calm it will be ok.',\n",
       " \"it's hard to stay clam. how do you do it? \",\n",
       " 'hi_comma_ this year_comma_ i was the first over 300 students at my enginering school',\n",
       " \"sounds great! so what's your major?\",\n",
       " 'it is computer science. i am very happy of this achievement and my family is very proud.',\n",
       " \"well pleased. you should be having brains_comma_man!that's a tough course_comma_i hear.\",\n",
       " 'during christmas a few years ago_comma_ i did not get any presents.',\n",
       " 'wow_comma_ that must be terrible_comma_ i cannot imagine_comma_ i lvoe christmas',\n",
       " 'since that day christmas has not been a good time for me. as i have no family_comma_ christmas is always the worst.',\n",
       " 'wow_comma_ i am sorry to hear that_comma_ i wish i could make it a better holiday for you!',\n",
       " 'some days are just days that you never forget. i wish you could as well.',\n",
       " 'my coworker is allowed to work remotely_comma_ but i am not...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Convert rows in dataframes to lower and add \n",
    "them to a sentence array\n",
    "--------------------------------------------\n",
    "'''\n",
    "word_arr = []\n",
    "for i in final_train_dataset.Attribute:\n",
    "    word_arr.append(i.lower())\n",
    "    \n",
    "#Sample\n",
    "word_arr[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00am</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100_comma_000</th>\n",
       "      <th>100f</th>\n",
       "      <th>100k</th>\n",
       "      <th>100x</th>\n",
       "      <th>10_comma_</th>\n",
       "      <th>10_comma_000</th>\n",
       "      <th>...</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zelda_comma_</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplining</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom_comma_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00am  10  100  1000  100_comma_000  100f  100k  100x  10_comma_  \\\n",
       "0     0   0    0     0              0     0     0     0          0   \n",
       "1     0   0    0     0              0     0     0     0          0   \n",
       "2     0   0    0     0              0     0     0     0          0   \n",
       "3     0   0    0     0              0     0     0     0          0   \n",
       "4     0   0    0     0              0     0     0     0          0   \n",
       "\n",
       "   10_comma_000     ...       zelda  zelda_comma_  zero  zimbabwe  zip  \\\n",
       "0             0     ...           0             0     0         0    0   \n",
       "1             0     ...           0             0     0         0    0   \n",
       "2             0     ...           0             0     0         0    0   \n",
       "3             0     ...           0             0     0         0    0   \n",
       "4             0     ...           0             0     0         0    0   \n",
       "\n",
       "   ziplining  zone  zoo  zoom  zoom_comma_  \n",
       "0          0     0    0     0            0  \n",
       "1          0     0    0     0            0  \n",
       "2          0     0    0     0            0  \n",
       "3          0     0    0     0            0  \n",
       "4          0     0    0     0            0  \n",
       "\n",
       "[5 rows x 9132 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Count Vectorizer to find B-O-W\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Count of the Occurances of the word\n",
    "count_occurs = vectorizer.fit_transform(word_arr)\n",
    "\n",
    "#Get the features/words\n",
    "features = vectorizer.get_feature_names()\n",
    "#print(features)\n",
    "feature_matrix = pd.DataFrame(count_occurs.toarray(),columns = features)\n",
    "\n",
    "#Sample\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     1\n",
       "10    1\n",
       "11    2\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "Name: christmas, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample occurances for the word christmas in each statement\n",
    "feature_matrix.christmas.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Remove Stopwords from the sentences\n",
    "--------------------------------------------\n",
    "'''\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#Remove Punctuation\n",
    "punct = list(string.punctuation)\n",
    "\n",
    "#Remove other unecessary words\n",
    "other_words = ['comma', 'am', 'pm','ugh', 'aagh', 'jeez','woah', 'oh', 'ooh', 'aall', 'ab', 'hmmm', 'zoo', 'zoos',\n",
    "               'you', 'yous','youve', 'yourself', 'zimbabwe', 'zealand','yursef','zip','zoom','ziplining', 'yyouca']\n",
    "for i in punct:\n",
    "    stop_words.append(i)\n",
    "for i in other_words:\n",
    "    stop_words.append(i)\n",
    "\n",
    "#Remove digits\n",
    "for i in range(len(word_arr)):\n",
    "    word_arr[i] = ''.join(c for c in word_arr[i] if not c.isdigit())\n",
    "for i in range(len(word_arr)):\n",
    "    word_arr[i] = ''.join(c for c in word_arr[i] if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Refine the words and genrated sentences\n",
    "--------------------------------------------\n",
    "'''\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "#Use wordNet Lemmatizer to lemmatize the words and find the stem words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_arr_no_stop_train = []\n",
    "for sentence in word_arr:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    sent_no_stop = \"\"\n",
    "    i = 0\n",
    "    for word in tokens:\n",
    "        i += 1\n",
    "        word = lemmatizer.lemmatize(word,'v')#Verb\n",
    "        word = lemmatizer.lemmatize(word,'n')#Noun\n",
    "        word = lemmatizer.lemmatize(word,'a')#Adjective\n",
    "        word = re.sub('[^a-z\\s]',\" \",word)\n",
    "        if word not in stop_words:\n",
    "            if i==1:\n",
    "                sent_no_stop +=''.join(word)\n",
    "            else:\n",
    "                sent_no_stop +=\" \"+''.join(word)\n",
    "    word_arr_no_stop_train.append(sent_no_stop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra spaces\n",
    "for i in range(len(word_arr_no_stop_train)):\n",
    "    word_arr_no_stop_train[i] = \" \".join(word_arr_no_stop_train[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words with one letter\n",
    "for i in range(len(word_arr_no_stop_train)):\n",
    "    word_arr_no_stop_train[i] = \" \".join(w for w in word_arr_no_stop_train[i].split() if len(w)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the word 'comma'\n",
    "for i in range(len(word_arr_no_stop_train)):\n",
    "    word_arr_no_stop_train[i] = \" \".join(w for w in word_arr_no_stop_train[i].split() if w!='comma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abandon', 'abduction', 'abilities', 'ability', 'able', 'abnout',\n",
       "       'abortion', 'about', 'abouts', 'abroad',\n",
       "       ...\n",
       "       'yuck', 'yukon', 'yum', 'yummy', 'yup', 'zelda', 'zero', 'zip', 'zone',\n",
       "       'zoom'],\n",
       "      dtype='object', length=5759)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Count Vectorizer to find B-O-W\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_occurs = vectorizer.fit_transform(word_arr_no_stop_train)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "#print(features)\n",
    "feature_matrix = pd.DataFrame(count_occurs.toarray(),columns = features)\n",
    "\n",
    "#Columns Sample\n",
    "feature_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     1\n",
       "10    1\n",
       "11    2\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "Name: christmas, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample\n",
    "feature_matrix.christmas.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Normalization</b><br>\n",
    "\n",
    "In order to achieve this we use TfIDF. In thi case we are trying bothe TDIDFVEctorizer and TFIDFTransfomer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abduction</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnout</th>\n",
       "      <th>abortion</th>\n",
       "      <th>about</th>\n",
       "      <th>abouts</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abduction  abilities  ability  able  abnout  abortion  about  \\\n",
       "0      0.0        0.0        0.0      0.0   0.0     0.0       0.0    0.0   \n",
       "1      0.0        0.0        0.0      0.0   0.0     0.0       0.0    0.0   \n",
       "2      0.0        0.0        0.0      0.0   0.0     0.0       0.0    0.0   \n",
       "3      0.0        0.0        0.0      0.0   0.0     0.0       0.0    0.0   \n",
       "4      0.0        0.0        0.0      0.0   0.0     0.0       0.0    0.0   \n",
       "\n",
       "   abouts  abroad  ...   yuck  yukon  yum  yummy  yup  zelda  zero  zip  zone  \\\n",
       "0     0.0     0.0  ...    0.0    0.0  0.0    0.0  0.0    0.0   0.0  0.0   0.0   \n",
       "1     0.0     0.0  ...    0.0    0.0  0.0    0.0  0.0    0.0   0.0  0.0   0.0   \n",
       "2     0.0     0.0  ...    0.0    0.0  0.0    0.0  0.0    0.0   0.0  0.0   0.0   \n",
       "3     0.0     0.0  ...    0.0    0.0  0.0    0.0  0.0    0.0   0.0  0.0   0.0   \n",
       "4     0.0     0.0  ...    0.0    0.0  0.0    0.0  0.0    0.0   0.0  0.0   0.0   \n",
       "\n",
       "   zoom  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 5759 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Tfidf Vectorizer normalie the occurances\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_vactorizer = TfidfVectorizer()\n",
    "tfidf_counts = tfidf_vactorizer.fit_transform(word_arr_no_stop_train)\n",
    "\n",
    "features = tfidf_vactorizer.get_feature_names()\n",
    "#print(features)\n",
    "feature_matrix = pd.DataFrame(tfidf_counts.toarray(),columns = features)\n",
    "\n",
    "#Print the normalized sample\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.000000\n",
       "1     0.000000\n",
       "2     0.000000\n",
       "3     0.000000\n",
       "4     0.000000\n",
       "5     0.000000\n",
       "6     0.000000\n",
       "7     0.000000\n",
       "8     0.000000\n",
       "9     0.520043\n",
       "10    0.417727\n",
       "11    0.740580\n",
       "12    0.000000\n",
       "13    0.000000\n",
       "14    0.000000\n",
       "Name: christmas, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized occurance count for the word christmas\n",
    "feature_matrix.christmas.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Label</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Job interviews always make me sweat bullets_co...</td>\n",
       "      <td>terrified</td>\n",
       "      <td>job interview always make sweat bullets make u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Don't be nervous. Just be prepared.</td>\n",
       "      <td>terrified</td>\n",
       "      <td>nervous prepare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I feel like getting prepared and then having a...</td>\n",
       "      <td>terrified</td>\n",
       "      <td>feel like get prepare curve ball throw throw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yes but if you stay calm it will be ok.</td>\n",
       "      <td>terrified</td>\n",
       "      <td>yes stay calm ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>It's hard to stay clam. How do you do it?</td>\n",
       "      <td>terrified</td>\n",
       "      <td>hard stay clam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Attribute      Label  \\\n",
       "21  Job interviews always make me sweat bullets_co...  terrified   \n",
       "22                Don't be nervous. Just be prepared.  terrified   \n",
       "23  I feel like getting prepared and then having a...  terrified   \n",
       "24            Yes but if you stay calm it will be ok.  terrified   \n",
       "25         It's hard to stay clam. How do you do it?   terrified   \n",
       "\n",
       "                                                Train  \n",
       "21  job interview always make sweat bullets make u...  \n",
       "22                                    nervous prepare  \n",
       "23       feel like get prepare curve ball throw throw  \n",
       "24                                   yes stay calm ok  \n",
       "25                                     hard stay clam  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_dataset['Train'] = word_arr_no_stop_train\n",
    "final_train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Repeat the above steps for the test dataset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Convert rows in dataframes to lower and add \n",
    "them to a sentence array\n",
    "--------------------------------------------\n",
    "'''\n",
    "word_arr = []\n",
    "for i in final_test_dataset.Attribute:\n",
    "    word_arr.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Remove Stopwords from the sentences\n",
    "--------------------------------------------\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "punct = list(string.punctuation)\n",
    "other_words = ['comma', 'am', 'pm', 'ugh', 'aagh', 'jeez','woah', 'oh', 'ooh', 'aall', 'ab', 'hmmm', 'zoo', 'zoos'\n",
    "                'you', 'yous','youve', 'yourself','zimbabwe', 'zealand','yursef','zip','zoom','ziplining', 'yyouca']\n",
    "for i in punct:\n",
    "    stop_words.append(i)\n",
    "for i in other_words:\n",
    "    stop_words.append(i)\n",
    "    \n",
    "for i in range(len(word_arr)):\n",
    "    word_arr[i] = ''.join(c for c in word_arr[i] if not c.isdigit())\n",
    "for i in range(len(word_arr)):\n",
    "    word_arr[i] = ''.join(c for c in word_arr[i] if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Refine the words and generated sentences\n",
    "--------------------------------------------\n",
    "'''\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "word_arr_no_stop = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_arr_no_stop = []\n",
    "for sentence in word_arr:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    sent_no_stop = \"\"\n",
    "    i = 0\n",
    "    for word in tokens:\n",
    "        i += 1\n",
    "        word = lemmatizer.lemmatize(word,'v')\n",
    "        word = lemmatizer.lemmatize(word,'n')\n",
    "        word = lemmatizer.lemmatize(word,'a')\n",
    "        word = re.sub('[^a-z\\s]',\" \",word)\n",
    "        if word not in stop_words:\n",
    "            if i==1:\n",
    "                sent_no_stop +=''.join(word)\n",
    "            else:\n",
    "                sent_no_stop +=\" \"+''.join(word)\n",
    "    word_arr_no_stop.append(sent_no_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "for i in range(len(word_arr_no_stop)):\n",
    "    word_arr_no_stop[i] = \" \".join(word_arr_no_stop[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove single letter words\n",
    "for i in range(len(word_arr_no_stop)):\n",
    "    word_arr_no_stop[i] = \" \".join(w for w in word_arr_no_stop[i].split() if len(w)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the word 'comma'\n",
    "for i in range(len(word_arr_no_stop)):\n",
    "    word_arr_no_stop[i] = \" \".join(w for w in word_arr_no_stop[i].split() if w!='comma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abandon', 'abilities', 'ability', 'able', 'absolutely', 'accent',\n",
       "       'accept', 'acceptance', 'accomplish', 'accomplishment',\n",
       "       ...\n",
       "       'yes', 'yesterday', 'yet', 'yikes', 'you', 'young', 'younger',\n",
       "       'yourself', 'youtube', 'zoos'],\n",
       "      dtype='object', length=2030)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Count Vectorizer to find B-O-W\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_occurs = vectorizer.fit_transform(word_arr_no_stop)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "#print(features)\n",
    "feature_matrix = pd.DataFrame(count_occurs.toarray(),columns = features)\n",
    "\n",
    "#Columns Sample\n",
    "feature_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>accomplishment</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yikes</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zoos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abilities  ability  able  absolutely  accent  accept  acceptance  \\\n",
       "0      0.0        0.0      0.0   0.0         0.0     0.0     0.0         0.0   \n",
       "1      0.0        0.0      0.0   0.0         0.0     0.0     0.0         0.0   \n",
       "2      0.0        0.0      0.0   0.0         0.0     0.0     0.0         0.0   \n",
       "3      0.0        0.0      0.0   0.0         0.0     0.0     0.0         0.0   \n",
       "4      0.0        0.0      0.0   0.0         0.0     0.0     0.0         0.0   \n",
       "\n",
       "   accomplish  accomplishment  ...        yes  yesterday  yet  yikes  you  \\\n",
       "0         0.0             0.0  ...   0.000000        0.0  0.0    0.0  0.0   \n",
       "1         0.0             0.0  ...   0.000000        0.0  0.0    0.0  0.0   \n",
       "2         0.0             0.0  ...   0.000000        0.0  0.0    0.0  0.0   \n",
       "3         0.0             0.0  ...   0.000000        0.0  0.0    0.0  0.0   \n",
       "4         0.0             0.0  ...   0.181151        0.0  0.0    0.0  0.0   \n",
       "\n",
       "   young  younger  yourself  youtube  zoos  \n",
       "0    0.0      0.0       0.0      0.0   0.0  \n",
       "1    0.0      0.0       0.0      0.0   0.0  \n",
       "2    0.0      0.0       0.0      0.0   0.0  \n",
       "3    0.0      0.0       0.0      0.0   0.0  \n",
       "4    0.0      0.0       0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 2030 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Tfidf Vectorizer normalize the occurances\n",
    "in the test dataset\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vactorizer = TfidfVectorizer()\n",
    "tfidf_counts = tfidf_vactorizer.fit_transform(word_arr_no_stop)\n",
    "\n",
    "features = tfidf_vactorizer.get_feature_names()\n",
    "\n",
    "#print(features)\n",
    "feature_matrix = pd.DataFrame(tfidf_counts.toarray(),columns = features)\n",
    "\n",
    "#Print the normalized sample\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Label</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I'm so sad because i've read an article about ...</td>\n",
       "      <td>sad</td>\n",
       "      <td>sad ve read article newborn girl die parent be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ugh_comma_ those articles always get me too......</td>\n",
       "      <td>sad</td>\n",
       "      <td>ugh article always get wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>she was born premature at home_comma_ she had ...</td>\n",
       "      <td>sad</td>\n",
       "      <td>bear premature home hard time breathe instead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jeez! Its so unfortunate... very sad really.</td>\n",
       "      <td>sad</td>\n",
       "      <td>unfortunate sad really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>yes! And i do believe in God and prayers but g...</td>\n",
       "      <td>sad</td>\n",
       "      <td>yes believe god prayer goodness gracious pleas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Attribute Label  \\\n",
       "17  I'm so sad because i've read an article about ...   sad   \n",
       "18  Ugh_comma_ those articles always get me too......   sad   \n",
       "19  she was born premature at home_comma_ she had ...   sad   \n",
       "20      Jeez! Its so unfortunate... very sad really.    sad   \n",
       "21  yes! And i do believe in God and prayers but g...   sad   \n",
       "\n",
       "                                                 Test  \n",
       "17  sad ve read article newborn girl die parent be...  \n",
       "18                       ugh article always get wrong  \n",
       "19  bear premature home hard time breathe instead ...  \n",
       "20                             unfortunate sad really  \n",
       "21  yes believe god prayer goodness gracious pleas...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_dataset['Test'] = word_arr_no_stop\n",
    "final_test_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. SGDClassifier</b><br>\n",
    "\n",
    "In order to achieve this we use TfIDF. In this case TfidfTransformer helps standardize the train and test dataset helping to fit the model and predict the labes for test dataset\n",
    "\n",
    "As we can see below we get and accuracy of <b>62.4%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------\n",
    "Using SGD Classifier\n",
    "--------------------------------------------\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "#Defining the train and test attributes and labels\n",
    "trainDocs,trainLabels = word_arr_no_stop_train, final_train_dataset.Label\n",
    "testDocs,testLabels = word_arr_no_stop, test_data.context\n",
    "\n",
    "#Use CountVectorizer and TfidfTransformer on the train dataset\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(trainDocs)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_array = X_train_tfidf.toarray()\n",
    "\n",
    "#Use CountVectorizer and TfidfTransformer on the test dataset\n",
    "X_test_counts = count_vect.transform(testDocs)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "\n",
    "X_test_array = X_test_tfidf.toarray()\n",
    "\n",
    "#Define the classifier and add parameters like alpha, max_iter, penalty etc.\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, tol=1e-3, max_iter=1000)\n",
    "\n",
    "#Fit the model on the train dataset\n",
    "clf.fit(X_train_tfidf,np.array(trainLabels))\n",
    "\n",
    "#Predict the values for teh test dataset and compare the results\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "acc_tfidf = np.mean(predicted == np.array(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:62.05%\n"
     ]
    }
   ],
   "source": [
    "#Below are the results\n",
    "print('Accuracy of the model is:'+ str(round(acc_tfidf*100,2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 \n",
    "1. Fine tuning the results means using the pretrained model to help us get better results by looking at the correlations to optimize the results. In our scenario we have the conversation chatbot. The average length of each conversation is close to 4. In this scenario we try to fine tune the results by checking the context window of the previous 4 utterances to make the model predict the context of the next utterance. This can help us improve the results by correlating to the context available making the prediction more accurate thus, helping the chatbot understand the context of the situation. However,finetuning needs to add weights to the previous occurances of the context to the most recent one\n",
    "<br>\n",
    "2. In a transformer normally each encoder is similar to one another and so are the decoders. Another property of transformers is that each word in the encoder has its own path. That is the processing and feed forward happens at a faster pace i.e. there are no dependencies thus, increasing the pace. Also encoder and decoder decide the weights of a word depending on the other words in the sentence. Correlating the words helps in undertsanding the context. These properties help transformer to provide better results for the empathetic chatbot\n",
    "<br>\n",
    "3. P@1,100: This is the accuracy of the model of choosing a correct response out of the given 100 randomly selected test set. <br>\n",
    "    AVG-BLEU: BLEU score is used to evaluate a generated senetence to a reference sentence. BLEU uses the computation of precision using n-gram for this and depending on the type of n-gram used we have BLEU-1-2-3-4. In this study we are using BLEU which also means the average BLEU for the results from various BLEU approaches<br>\n",
    "    PPL: Perplexity - Perplexity is an exponentiation of entropy. This captures the uncertainity in the model. Higher the entropy higher the perplexity and vice-versa\n",
    "<br>\n",
    "4. BLEU seems to the best metric as it used the correration of words to generate the meteric. As we can see BLEU-n uses the computation precision of the n-gram thus helping in correlating the words that often fall together. This is really important in the empathetic systems to undertstand the context\n",
    "<br>\n",
    "5. Currently the model is empathetic but not as empathetic as humans. To improve this we need to feed the model with more training dataset that can help in incorporating the same in general dialog. The improvements can be done with data and capacity. In case we have better. We can also improve the tuning model to take ito account the similarities in a context and also using the human trained pretrained dataset to help identify and fine tune it more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cpu: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "import gensim.models\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "workers = multiprocessing.cpu_count()\n",
    "print('number of cpu: {}'.format(workers))\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10939\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_sents = [word_tokenize(i) for i in final_train_dataset['Train']]\n",
    "print(len(tokenized_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Word2Vec(tokenized_sents,\n",
    "                      min_count=2,\n",
    "                      size=100,\n",
    "                      window=5,\n",
    "                      workers=workers,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "\n",
    "        self.word_model = word_model\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        \"\"\"\n",
    "        Fit in a list of docs, which had been preprocessed and tokenized,\n",
    "        such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
    "        Then build up a tfidf model to compute each word's idf as its weight.\n",
    "        Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
    "        :param\n",
    "            pre_processed_docs: list of docs, which are tokenized\n",
    "        :return:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        text_docs = []\n",
    "        for doc in docs:\n",
    "            text_docs.append(\" \".join(doc))\n",
    "\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(text_docs)  # must be list of text string\n",
    "\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "        self.word_idf_weight = defaultdict(lambda: max_idf,\n",
    "                                               [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            #logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_tr  = TfidfEmbeddingVectorizer(word_model)\n",
    "  # fit tfidf model first\n",
    "\n",
    "tfidf_vec_tr.fit(tokenized_sents) \n",
    "tfidf_doc_vec = tfidf_vec_tr.transform(tokenized_sents)\n",
    "\n",
    "# print('Demo of word averaging doc vector...')\n",
    "# display(doc_vec[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf-word-mean doc2vec...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10939, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.07985463,   3.65502405,  -0.54506034, ...,  -4.94287586,\n",
       "          2.27880907,  -1.65137064],\n",
       "       [ -8.77450943,   0.79331565,   4.41261625, ...,  -2.16055083,\n",
       "        -10.44566917,   5.73236275],\n",
       "       [-10.17860603,   0.05264745,   4.33068275, ...,  -2.35055733,\n",
       "         -6.40414047,  -0.86498821],\n",
       "       ...,\n",
       "       [  4.95237494,  -9.69329166,   1.46142483, ...,  -0.03043954,\n",
       "         -2.97034621,  -3.80776119],\n",
       "       [  4.03734732,  -0.87526542,  -1.5348947 , ...,  -2.48545265,\n",
       "          1.6004467 ,   3.0723021 ],\n",
       "       [  0.15700443,  -3.01226926,  -1.27088082, ...,   0.26645681,\n",
       "          3.87873197,   2.34608459]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save word averaging doc2vec.\n",
    "print('Shape of tfidf-word-mean doc2vec...')\n",
    "display(tfidf_doc_vec.shape)\n",
    "\n",
    "tfidf_doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_test_sents = [word_tokenize(i) for i in final_test_dataset['Test']]\n",
    "print(len(tokenized_test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc_vec_test = tfidf_vec_tr.transform(tokenized_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf-word-mean doc2vec...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1349, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.59118581,  0.75729626, -0.7386176 , ...,  2.6746769 ,\n",
       "        -3.42214346,  5.54761124],\n",
       "       [-0.26679391,  1.57986748, -2.63601422, ..., -3.4692893 ,\n",
       "        -3.5446744 , -0.02520895],\n",
       "       [-1.78760719,  2.11584401,  0.79802573, ...,  5.86673737,\n",
       "        -1.70027959,  4.44860125],\n",
       "       ...,\n",
       "       [-3.13985419, -0.07907978, -1.02455497, ...,  0.30550432,\n",
       "        -3.24897647, -3.78489423],\n",
       "       [ 3.96508527,  5.43052292,  2.07830215, ..., -4.03631878,\n",
       "         4.05159283,  4.79649544],\n",
       "       [-1.52722943, -0.51442164, -1.34763908, ..., -2.00467086,\n",
       "         3.65751672, -2.6253159 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save word averaging doc2vec.\n",
    "print('Shape of tfidf-word-mean doc2vec...')\n",
    "display(tfidf_doc_vec_test.shape)\n",
    "\n",
    "tfidf_doc_vec_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 100, Bias: -0.823357, T: 10939, Avg. loss: 6.883600\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.94, NNZs: 100, Bias: -1.007264, T: 21878, Avg. loss: 1.895180\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.86, NNZs: 100, Bias: -1.020977, T: 32817, Avg. loss: 1.519462\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.64, NNZs: 100, Bias: -1.046844, T: 43756, Avg. loss: 1.355339\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.42, NNZs: 100, Bias: -1.013220, T: 54695, Avg. loss: 1.255594\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.48, NNZs: 100, Bias: -1.046863, T: 65634, Avg. loss: 1.153463\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.30, NNZs: 100, Bias: -1.063595, T: 76573, Avg. loss: 1.110509\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.21, NNZs: 100, Bias: -1.028193, T: 87512, Avg. loss: 1.022082\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.10, NNZs: 100, Bias: -1.021690, T: 98451, Avg. loss: 1.010531\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.09, NNZs: 100, Bias: -0.994574, T: 109390, Avg. loss: 0.974289\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.16, NNZs: 100, Bias: -1.053024, T: 120329, Avg. loss: 0.953993\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.11, NNZs: 100, Bias: -1.026553, T: 131268, Avg. loss: 0.936543\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.95, NNZs: 100, Bias: -1.005072, T: 142207, Avg. loss: 0.909166\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.04, NNZs: 100, Bias: -1.038778, T: 153146, Avg. loss: 0.898477\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.85, NNZs: 100, Bias: -0.996110, T: 164085, Avg. loss: 0.875703\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.88, NNZs: 100, Bias: -1.011345, T: 175024, Avg. loss: 0.865950\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.94, NNZs: 100, Bias: -1.025628, T: 185963, Avg. loss: 0.841726\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.90, NNZs: 100, Bias: -1.014491, T: 196902, Avg. loss: 0.831188\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.83, NNZs: 100, Bias: -1.016970, T: 207841, Avg. loss: 0.825949\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.79, NNZs: 100, Bias: -1.032154, T: 218780, Avg. loss: 0.821493\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.86, NNZs: 100, Bias: -1.040563, T: 229719, Avg. loss: 0.818136\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.89, NNZs: 100, Bias: -1.025943, T: 240658, Avg. loss: 0.803214\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.82, NNZs: 100, Bias: -1.017832, T: 251597, Avg. loss: 0.788478\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.83, NNZs: 100, Bias: -1.019747, T: 262536, Avg. loss: 0.777922\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.69, NNZs: 100, Bias: -1.019680, T: 273475, Avg. loss: 0.765418\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.75, NNZs: 100, Bias: -0.991326, T: 284414, Avg. loss: 0.764888\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.73, NNZs: 100, Bias: -1.012092, T: 295353, Avg. loss: 0.750206\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.77, NNZs: 100, Bias: -1.013940, T: 306292, Avg. loss: 0.766240\n",
      "Total training time: 0.26 seconds.\n",
      "Convergence after 28 epochs took 0.26 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.98, NNZs: 100, Bias: -0.243297, T: 10939, Avg. loss: 6.462111\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.52, NNZs: 100, Bias: -0.428618, T: 21878, Avg. loss: 1.946374\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.94, NNZs: 100, Bias: -0.489910, T: 32817, Avg. loss: 1.524960\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.73, NNZs: 100, Bias: -0.638601, T: 43756, Avg. loss: 1.379638\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.61, NNZs: 100, Bias: -0.697717, T: 54695, Avg. loss: 1.236174\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.52, NNZs: 100, Bias: -0.746338, T: 65634, Avg. loss: 1.142760\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.39, NNZs: 100, Bias: -0.780208, T: 76573, Avg. loss: 1.097235\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.26, NNZs: 100, Bias: -0.804969, T: 87512, Avg. loss: 1.050049\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.16, NNZs: 100, Bias: -0.890887, T: 98451, Avg. loss: 0.987953\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.10, NNZs: 100, Bias: -0.946014, T: 109390, Avg. loss: 0.996523\n",
      "Total training time: 0.09 seconds.\n",
      "Convergence after 10 epochs took 0.09 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.83, NNZs: 100, Bias: -1.007508, T: 10939, Avg. loss: 6.205147\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.40, NNZs: 100, Bias: -1.102563, T: 21878, Avg. loss: 2.158196\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.02, NNZs: 100, Bias: -1.072701, T: 32817, Avg. loss: 1.749336\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.65, NNZs: 100, Bias: -1.018529, T: 43756, Avg. loss: 1.553033\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.61, NNZs: 100, Bias: -1.009492, T: 54695, Avg. loss: 1.434756\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.38, NNZs: 100, Bias: -1.073454, T: 65634, Avg. loss: 1.343798\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.32, NNZs: 100, Bias: -1.008434, T: 76573, Avg. loss: 1.286901\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.11, NNZs: 100, Bias: -0.992305, T: 87512, Avg. loss: 1.211946\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 100, Bias: -1.003087, T: 98451, Avg. loss: 1.170490\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.02, NNZs: 100, Bias: -1.022516, T: 109390, Avg. loss: 1.128427\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.09, NNZs: 100, Bias: -1.025675, T: 120329, Avg. loss: 1.076680\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.99, NNZs: 100, Bias: -1.014730, T: 131268, Avg. loss: 1.049239\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.01, NNZs: 100, Bias: -1.030599, T: 142207, Avg. loss: 1.028396\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.98, NNZs: 100, Bias: -1.009593, T: 153146, Avg. loss: 1.000630\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.99, NNZs: 100, Bias: -1.024408, T: 164085, Avg. loss: 1.003386\n",
      "Total training time: 0.15 seconds.\n",
      "Convergence after 15 epochs took 0.15 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.03, NNZs: 100, Bias: -0.204953, T: 10939, Avg. loss: 6.670269\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.19, NNZs: 100, Bias: -0.433000, T: 21878, Avg. loss: 1.692950\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.07, NNZs: 100, Bias: -0.580040, T: 32817, Avg. loss: 1.388701\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.54, NNZs: 100, Bias: -0.710813, T: 43756, Avg. loss: 1.223529\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.55, NNZs: 100, Bias: -0.946469, T: 54695, Avg. loss: 1.136381\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.21, NNZs: 100, Bias: -0.964984, T: 65634, Avg. loss: 1.035121\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.48, NNZs: 100, Bias: -0.981967, T: 76573, Avg. loss: 0.971009\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.21, NNZs: 100, Bias: -1.023312, T: 87512, Avg. loss: 0.943324\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.08, NNZs: 100, Bias: -0.987298, T: 98451, Avg. loss: 0.896858\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.15, NNZs: 100, Bias: -1.009151, T: 109390, Avg. loss: 0.875910\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.03, NNZs: 100, Bias: -0.980177, T: 120329, Avg. loss: 0.842965\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.06, NNZs: 100, Bias: -1.000056, T: 131268, Avg. loss: 0.819903\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.97, NNZs: 100, Bias: -1.005416, T: 142207, Avg. loss: 0.817794\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.08, NNZs: 100, Bias: -1.002522, T: 153146, Avg. loss: 0.783097\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.96, NNZs: 100, Bias: -1.007312, T: 164085, Avg. loss: 0.778078\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.92, NNZs: 100, Bias: -1.014357, T: 175024, Avg. loss: 0.759849\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.94, NNZs: 100, Bias: -0.983667, T: 185963, Avg. loss: 0.738443\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.84, NNZs: 100, Bias: -0.977375, T: 196902, Avg. loss: 0.742228\n",
      "Total training time: 0.16 seconds.\n",
      "Convergence after 18 epochs took 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=1, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='hinge', max_iter=10000,\n",
       "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=1,\n",
       "       shuffle=True, tol=1e-07, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# (Optional) Classification via stochastic gradient descent classifier.\n",
    "sgd = SGDClassifier(loss='hinge',\n",
    "                    penalty='l2',\n",
    "                    verbose=1,\n",
    "                    random_state=1,\n",
    "                    learning_rate='invscaling',\n",
    "                    eta0=1, alpha=0.0001, tol=1e-7, max_iter=10000)\n",
    "\n",
    "sgd.fit(tfidf_doc_vec,final_train_dataset.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.5444739007221867\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    jealous       0.49      0.50      0.50      2546\n",
      "     joyful       0.53      0.54      0.54      2553\n",
      "        sad       0.53      0.52      0.52      3000\n",
      "  terrified       0.63      0.61      0.62      2840\n",
      "\n",
      "avg / total       0.55      0.54      0.54     10939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "train_pred = sgd.predict(tfidf_doc_vec)\n",
    "print('Training accuracy:'+str(accuracy_score(train_pred,final_train_dataset.Label)))\n",
    "print(classification_report(train_pred,final_train_dataset.Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.519644180874722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    jealous       0.41      0.47      0.44       301\n",
      "     joyful       0.46      0.54      0.50       301\n",
      "        sad       0.55      0.51      0.53       397\n",
      "  terrified       0.68      0.56      0.61       350\n",
      "\n",
      "avg / total       0.53      0.52      0.52      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = sgd.predict(tfidf_doc_vec_test)\n",
    "print('Training accuracy:'+str(accuracy_score(test_pred,final_test_dataset.Label)))\n",
    "print(classification_report(test_pred,final_test_dataset.Label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(64, 128, 512, 512, 1024, 64, 32),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=1000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=0.01, hidden_layer_sizes=(64,128,512,512,1024,64,32), max_iter=1000, activation='relu', solver='adam')\n",
    "\n",
    "#train_and_evaluate(mlp,X_train_w2v,final_train_dataset.Label,X_test_w2v,final_test_dataset.Label)\n",
    "mlp.fit(tfidf_doc_vec,final_train_dataset.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.9666331474540635\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    jealous       0.96      0.98      0.97      2527\n",
      "     joyful       0.98      0.94      0.96      2729\n",
      "        sad       0.96      0.97      0.97      2945\n",
      "  terrified       0.97      0.98      0.97      2738\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mlp_pred = mlp.predict(tfidf_doc_vec)\n",
    "print('Training accuracy:'+str(accuracy_score(train_mlp_pred,final_train_dataset.Label)))\n",
    "print(classification_report(train_mlp_pred,final_train_dataset.Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:0.5189028910303929\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    jealous       0.42      0.52      0.47       274\n",
      "     joyful       0.53      0.53      0.53       346\n",
      "        sad       0.52      0.49      0.51       393\n",
      "  terrified       0.62      0.53      0.57       336\n",
      "\n",
      "avg / total       0.53      0.52      0.52      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mlp_pred = mlp.predict(tfidf_doc_vec_test)\n",
    "print('Training accuracy:'+str(accuracy_score(test_mlp_pred,final_test_dataset.Label)))\n",
    "print(classification_report(test_mlp_pred,final_test_dataset.Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
